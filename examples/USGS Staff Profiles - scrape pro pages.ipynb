{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there is also no way of programmatically getting at the information that USGS staff have put together for their profile pages, so I had to come up with a scraper for that information. I put this into a single function that operates against a given staff profile URL and am currently limiting the work to anything that we can't get from other sources. The interesting bits include a set of keywords, drawn from the USGS Thesaurus, that represent self-assertions of expertise. The other main section that an individual can control themselves is a big body of HTML. We might be able to work through this content with further processing, but for now, I shove that into a string and extract all of the links from it in a list. The links often include publications that are not otherwise part of the listing coming from official channels, which can be a very useful addendum to a research record.\n",
    "\n",
    "As an example of working with this capability, this notebook picks up on some work that Leslie Hsu started for the USGS Community for Data Integration. We focus in on CDI members from USGS with staff profiles, scrape up all of their expertise keywords, and then work through the results. There's more to do here in terms of analysis with the keywords that we'll get into elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylinkedcmd.pylinkedcmd as l_cmd\n",
    "from joblib import Parallel, delayed\n",
    "import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from sqlite_utils import Database\n",
    "\n",
    "usgs_web = l_cmd.UsgsWeb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDI Members are all part of a group in Confluence used for the CDI wiki. Unfortunately, the API for that is not public so I had to build this in as an authenticated API connection to get the group membership. As far as I know, anyone with a myUSGS account should be able to run this. I then get just CDI members who have usgs.gov email addresses (usernames) and figure out which ones have listings in our profiles data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "def get_group(group_name, source=\"file\", file_name=\"cdi-users.json\", filter_usgs=True):\n",
    "    if source == \"file\" and os.path.exists(file_name):\n",
    "        with open(file_name, \"r\") as f:\n",
    "            results=json.load(f)\n",
    "            f.close()\n",
    "    else:\n",
    "        session = requests.Session()\n",
    "        session.headers.update({'Accept': 'application/json'})\n",
    "        session.headers.update({'Content-Type': 'application/json'}) \n",
    "\n",
    "        digest = HTTPBasicAuth(input(\"User Name: \"), getpass())\n",
    "\n",
    "        conf_url = f\"https://my.usgs.gov/confluence/rest/api/group/{group_name}/member\"\n",
    "\n",
    "        results = session.get(conf_url, auth=digest).json()\n",
    "        \n",
    "    user_list = [\n",
    "        {\n",
    "            \"email\": i[\"username\"], \n",
    "            \"name\": i[\"displayName\"]\n",
    "        } for i in results[\"results\"]\n",
    "    ]\n",
    "    \n",
    "    if filter_usgs:\n",
    "        user_list = [i for i in user_list if i[\"email\"].split(\"@\")[-1] in [\"usgs.gov\",\"contractor.usgs.gov\"]]\n",
    "\n",
    "    return user_list\n",
    "\n",
    "def get_user_url(user_email):\n",
    "    r = requests.get(f\"https://www.sciencebase.gov/directory/people?format=json&email={user_email}\").json()\n",
    "    if len(r[\"people\"]) != 1:\n",
    "        return None\n",
    "\n",
    "    return r[\"people\"][0][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.5 s, sys: 211 ms, total: 2.71 s\n",
      "Wall time: 44.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cdi_members_usgs = [dict(item, url=get_user_url(item[\"email\"])) for item in get_group(\"cdi-users\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a quick look at the numbers, we have 25 CDI members with USGS user names who do not appear to have staff profile pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CDI members from USGS:  131\n",
      "USGS CDI members with URLs:  106\n"
     ]
    }
   ],
   "source": [
    "cdi_members_usgs_staff_profiles = [\n",
    "    i for i in cdi_members_usgs if i[\"url\"] is not None and i[\"url\"].find(\"/staff-profiles/\") > 0\n",
    "]\n",
    "\n",
    "cdi_member_profile_urls = [i[\"url\"] for i in cdi_members_usgs_staff_profiles]\n",
    "\n",
    "print(\"Total CDI members from USGS: \", len(cdi_members_usgs))\n",
    "print(\"USGS CDI members with URLs: \", len(cdi_members_usgs_staff_profiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accumulator function will build a list of dictionaries containing member profiles from our parallel process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdi_member_profiles = list()\n",
    "\n",
    "def accumulator(url):\n",
    "    cdi_member_profiles.append(usgs_web.scrape_profile(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [01:19<00:00,  1.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parallel(n_jobs=10, prefer=\"threads\")(\n",
    "    delayed(accumulator)\n",
    "    (\n",
    "        i\n",
    "    ) for i in tqdm.tqdm(cdi_member_profile_urls)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Table staff_profiles (profile, display_name, profile_image_url, organization_name, organization_link, email, orcid, body_content_links, scraped_body_html, expertise)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = Database(\"usgs_profiles.db\")\n",
    "db[\"staff_profiles\"].upsert_all(cdi_member_profiles, pk=\"profile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to do some more work with expertise terms, so we pull those out here into their own table for further processing. I'm experimenting with harmonizing expertise terms, alignment with vocabulary assets, and extraction of additional terms from referenced sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Table expertise_terms (term_source, source_identifier, term, identifier)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expertise_terms = list()\n",
    "for item in [i for i in cdi_member_profiles if \"expertise\" in i.keys() and len(i[\"expertise\"]) > 0]:\n",
    "    for term in item[\"expertise\"]:\n",
    "        for t in term.split(\",\"):\n",
    "            d_term = {\n",
    "                \"term_source\": \"USGS Staff Profiles\",\n",
    "                \"source_identifier\": item[\"profile\"],\n",
    "                \"term\": t.strip()\n",
    "            }\n",
    "            d_term[\"identifier\"] = \":\".join(v for k,v in d_term.items())\n",
    "            expertise_terms.append(d_term)\n",
    "db[\"expertise_terms\"].upsert_all(expertise_terms, pk=\"identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
