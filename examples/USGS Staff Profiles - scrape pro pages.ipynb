{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there is also no way of programmatically getting at the information that USGS staff have put together for their profile pages, so I had to come up with a scraper for that information. I put this into a single function that operates against a given staff profile URL and am currently limiting the work to anything that we can't get from other sources. The interesting bits include a set of keywords, drawn from the USGS Thesaurus, that represent self-assertions of expertise. The other main section that an individual can control themselves is a big body of HTML. We might be able to work through this content with further processing, but for now, I shove that into a string and extract all of the links from it in a list. The links often include publications that are not otherwise part of the listing coming from official channels, which can be a very useful addendum to a research record.\n",
    "\n",
    "As an example of working with this capability, this notebook picks up on some work that Leslie Hsu started for the USGS Community for Data Integration. We focus in on CDI members from USGS with staff profiles, scrape up all of their expertise keywords, and then work through the results. There's more to do here in terms of analysis with the keywords that we'll get into elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylinkedcmd.pylinkedcmd as l_cmd\n",
    "from joblib import Parallel, delayed\n",
    "import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter\n",
    "\n",
    "usgs_web = l_cmd.UsgsWeb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to USGS Staff Profiles - scrape inventory on how to get the full listing of staff, most of whom have profile pages. This involves scraping a paginated listing of staff from over 280 pages. It doesn't take that long running multiple requests in parallel but it's reasonable to cache the information anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"usgs_staff.pkl\", \"rb\")\n",
    "usgs_staff = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDI Members are all part of a group in Confluence used for the CDI wiki. Unfortunately, the API for that is not public so I had to build this in as an authenticated API connection to get the group membership. As far as I know, anyone with a myUSGS account should be able to run this. I then get just CDI members who have usgs.gov email addresses (usernames) and figure out which ones have listings in our profiles data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "def get_group(group_name):\n",
    "    session = requests.Session()\n",
    "    session.headers.update({'Accept': 'application/json'})\n",
    "    session.headers.update({'Content-Type': 'application/json'}) \n",
    "\n",
    "    digest = HTTPBasicAuth(input(\"User Name: \"), getpass())\n",
    "\n",
    "    conf_url = f\"https://my.usgs.gov/confluence/rest/api/group/{group_name}/member\"\n",
    "    \n",
    "    results = session.get(conf_url, auth=digest).json()\n",
    "    \n",
    "    return results\n",
    "\n",
    "cdi_members = get_group(\"cdi-users\")\n",
    "    \n",
    "cdi_members_usgs = [\n",
    "    {\n",
    "        \"email\": i[\"username\"], \n",
    "        \"name\": i[\"displayName\"]\n",
    "    } for i in cdi_members[\"results\"] if i[\"username\"].split(\"@\")[-1] == \"usgs.gov\"\n",
    "]\n",
    "\n",
    "cdi_member_staff = [\n",
    "    i for i in usgs_staff\n",
    "    if next((e for e in i[\"email\"] if e is not None and e in [m[\"email\"] for m in cdi_members_usgs]), None) is not None\n",
    "]\n",
    "\n",
    "cdi_member_profile_urls = [i[\"profile\"][0] for i in cdi_member_staff if i[\"profile\"][0] is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a quick look at the numbers, we have 10 CDI members with USGS user names who were not in the staff listing. These are possibly former USGS staff. Of those, there is only one that does not have a profile page. So, we have a lot to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"USGS CDI Members\", len(cdi_members_usgs))\n",
    "print(\"USGS CDI Members from USGS staff listing\", len(cdi_member_staff))\n",
    "print(\"USGS CDI Members with profile paes\", len(cdi_member_profile_urls))\n",
    "print(\"Members with more than one profile page\", len([i for i in cdi_member_staff if len(i[\"profile\"]) > 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accumulator function will build a list of dictionaries containing member profiles from our parallel process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdi_member_profiles = list()\n",
    "\n",
    "def accumulator(url):\n",
    "    cdi_member_profiles.append(usgs_web.scrape_profile(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(n_jobs=10, prefer=\"threads\")(\n",
    "    delayed(accumulator)\n",
    "    (\n",
    "        i\n",
    "    ) for i in tqdm.tqdm(cdi_member_profile_urls)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pull out just the raw expertise keywords into their own list for a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "expertise_keywords = list()\n",
    "[expertise_keywords.extend(i[\"expertise\"]) for i in cdi_member_profiles if \"expertise\" in i.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expertise_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without further work to associate expertise keywords with the USGS Thesaurus or other vocabulary resources, the list is a bit crude. But we can give it a quick look with a wordcloud just to visualize raw words coming together so far. This is all words and not keyword phrases at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cloud(wordcloud):\n",
    "    plt.figure(figsize=(40, 30))\n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\");\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width = 3000, \n",
    "    height = 2000, \n",
    "    random_state=1, \n",
    "    background_color='salmon', \n",
    "    colormap='Pastel1', \n",
    "    collocations=False, \n",
    "    stopwords = STOPWORDS\n",
    ").generate_from_frequencies(Counter(expertise_keywords))\n",
    "\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
