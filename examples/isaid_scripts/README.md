# iSAID Data Processing

The scripts in this folder provide a rough idea toward the start of a data processing pipeline. After experimenting around with a number of ways of bringing these data together, I focused on getting everything relatively well flattened out to PostgreSQL tables and a couple of views to stitch things together. Those are then piped through a Hasura instance to provide a GraphQL end point for working with the data. Most of these processes run without a hitch, but we do need to move to a message queue/FaaS type of framework for both the couple of issues with interruptions and for regular updates.

The cache_pw_batches script is the one kind of problematic piece of this as the REST API suffers from regular disruptions. I had to batch things out by year, cache raw responses to files, and then process those in batches. So, that's obviously not ideal. What I hope to do is federate an experimental GraphQL end point for pubs to the iSAID end point to support the type of information synthesis operations we are running here. I do also optimize the records for examining linkages between authors and coauthors, coauthor affiliations, and cost centers as part of the summarization process.
