{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylinkedcmd import usgsweb\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import tqdm\n",
    "import requests\n",
    "from collections import Counter\n",
    "from iteration_utilities import unique_everseen\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "usgs_web = usgsweb.UsgsWeb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In working through the data, I found cases where profiles have been created that reference people but use email addresses that essentially mean we have no way of getting any further details about those people as individuals. This means we really don't need to pull their records into our process. I set those here as external to the codebase, but we can think about that differently as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_id_emails = [\n",
    "    \"ask@usgs.gov\",\n",
    "    \"astro_outreach@usgs.gov\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function from the usgsweb module gets the current maximum page in the Drupal-based pagination structure that is set up for the full profile listing and tees up the individual URLs to be hit in the inventory scraping routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://www.usgs.gov/connect/staff-profiles?page=0',\n",
       " 'https://www.usgs.gov/connect/staff-profiles?page=1',\n",
       " 'https://www.usgs.gov/connect/staff-profiles?page=2',\n",
       " 'https://www.usgs.gov/connect/staff-profiles?page=3',\n",
       " 'https://www.usgs.gov/connect/staff-profiles?page=4']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inventory_urls = usgs_web.get_staff_inventory_pages()\n",
    "print(len(inventory_urls))\n",
    "display(inventory_urls[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going after basic web pages, it is reasonable to parallelize the process to grab up at least a few pages at a time for processing. In the next two code blocks I set up an accumulator function and then run a number of threads to run the scraper on the inventory URLs. Everything goes into an in memory list of dictionaries called staff_inventory.\n",
    "\n",
    "When shifting to a message queue/lambda architecture for this, the URLs generated above become messages on the queue, and then lambda handles the multiprocessing. We will then need to look to some other data store to house the information scraped from the inventory (more on that below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff_inventory = list()\n",
    "\n",
    "def accumulator(url):\n",
    "    staff_list = usgs_web.get_staff_listing(url)\n",
    "    if isinstance(staff_list, list):\n",
    "        staff_inventory.extend([i for i in staff_list if \"email\" in i and i[\"email\"] not in non_id_emails])\n",
    "    else:\n",
    "        print(type(staff_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:42<00:00,  6.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parallel(n_jobs=20, prefer=\"threads\")(\n",
    "    delayed(accumulator)\n",
    "    (\n",
    "        i\n",
    "    ) for i in tqdm.tqdm(inventory_urls)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main thing the inventory process gives us at this point is the basic identification of a staff member and the link to their profile page. The name, title, organization name/link, email, and telephone number are all useful bits of information (though I'm not retaining telephone at this point - who uses phones for a voice call anymore?), but we also get those from the profile page themselves.\n",
    "\n",
    "In a message/lambda architecture, these individual records could go on another queue for scraping the actual individual profile pages, but note the limitation documented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'Melanie Addington',\n",
       "  'title': 'Engineering Technician',\n",
       "  'organization_name': 'USGS Water Resources Mission Area',\n",
       "  'organization_link': 'https://www.usgs.gov/mission-areas/water-resources',\n",
       "  'email': 'maddingt@usgs.gov',\n",
       "  'profile': 'https://usgs.gov/staff-profiles/melanie-addington',\n",
       "  'telephone': '228-688-1960'},\n",
       " {'name': 'Jason A Addison, Ph.D.',\n",
       "  'title': 'Research Geologist',\n",
       "  'organization_name': 'Geology, Minerals, Energy, and Geophysics Science Center',\n",
       "  'organization_link': 'https://www.usgs.gov/centers/gmeg',\n",
       "  'email': 'jaddison@usgs.gov',\n",
       "  'profile': 'https://usgs.gov/staff-profiles/jason-a-addison',\n",
       "  'telephone': '650-329-5271'},\n",
       " {'name': 'Songie Adebiyi',\n",
       "  'title': 'Administrative Officer',\n",
       "  'organization_name': 'Western Fisheries Research Center',\n",
       "  'organization_link': 'https://www.usgs.gov/centers/wfrc',\n",
       "  'email': 'sadebiyi@usgs.gov',\n",
       "  'profile': 'https://usgs.gov/staff-profiles/songie-adebiyi',\n",
       "  'telephone': '206-526-6287'},\n",
       " {'name': 'Mitchell B Adelson',\n",
       "  'title': 'Science Information Services',\n",
       "  'organization_name': 'Office of Communications and Publishing',\n",
       "  'organization_link': 'https://www.usgs.gov/about/organization/science-support/communications-and-publi...',\n",
       "  'email': 'madelson@usgs.gov',\n",
       "  'profile': 'https://usgs.gov/staff-profiles/mitchell-b-adelson',\n",
       "  'telephone': '650-329-4293'},\n",
       " {'name': 'Karen F Adkins',\n",
       "  'title': 'Senior Hydrography Project Lead',\n",
       "  'organization_name': 'National Geospatial Technical Operations Center (NGTOC)',\n",
       "  'organization_link': 'https://www.usgs.gov/core-science-systems/ngp/ngtoc',\n",
       "  'email': 'kadkins@usgs.gov',\n",
       "  'profile': 'https://usgs.gov/staff-profiles/karen-f-adkins',\n",
       "  'telephone': '303-202-4394'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(staff_inventory))\n",
    "display(staff_inventory[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the entire inventory together, I found that there are some problems in the underlying profile system where multiple profile pages/records have been created for what is determined to be the same person based on uniqueness of emails within this collection. In the following process, I determine where these cases exist and then run a parallel process to get the overall length of the content from the individual pages assigned to an individual. I use this to determine which profile URL we want to treat as the right one for each given case. It's stupid to have to do this, and I hope the folks running the Profiles system will clean things up eventually.\n",
    "\n",
    "This may create a problem for us in running everything to this point in an individual message paradigm as we would have no way of determining the specific URL among several to be used until we have everything all together. The processes are simple and quick enough not to exceed AWS Lambda limits, though, so we could run everything through this next point as a single process before building another queue of the actual profile page URLs we need to process fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_on_profile = [i for i in unique_everseen(staff_inventory)]\n",
    "profile_records = list()\n",
    "check_url_content_length = list()\n",
    "for email, count in Counter([i[\"email\"] for i in unique_on_profile]).most_common():\n",
    "    if count > 1:\n",
    "        check_url_content_length.extend([i[\"profile\"] for i in unique_on_profile if i[\"email\"] == email])\n",
    "    else:\n",
    "        profile_records.extend([i for i in unique_on_profile if i[\"email\"] == email])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_headers = list()\n",
    "\n",
    "def header_accumulator(url):\n",
    "    url_headers.append({\n",
    "        \"url\": url,\n",
    "        \"content-length\": len(requests.get(url).content)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:15<00:00, 12.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parallel(n_jobs=20, prefer=\"threads\")(\n",
    "    delayed(header_accumulator)\n",
    "    (\n",
    "        i\n",
    "    ) for i in tqdm.tqdm(check_url_content_length)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the results of getting content length, this is the process I worked out for determining which profile to actually assign to a given logical entity. The end result is a set of profile records that could now go on a queue for individual asynchronous processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for email, count in Counter([i[\"email\"] for i in unique_on_profile]).most_common():\n",
    "    if count > 1:\n",
    "        common_urls = [i[\"profile\"] for i in unique_on_profile if i[\"email\"] == email]\n",
    "        max_size = max([i[\"content-length\"] for i in url_headers if i[\"url\"] in common_urls])\n",
    "        best_url = next((i[\"url\"] for i in url_headers if i[\"url\"] in common_urls and i[\"content-length\"] == max_size), None)\n",
    "        profile_records.append(\n",
    "            next((i for i in unique_on_profile if i[\"profile\"] == best_url), None)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'Pamela Agnew',\n",
       "  'title': 'Workforce Data and Analysis Program Manager',\n",
       "  'organization_name': 'Administration',\n",
       "  'organization_link': 'https://www.usgs.gov/about/organization/science-support/human-capital',\n",
       "  'email': 'pagnew@usgs.gov',\n",
       "  'profile': 'https://usgs.gov/staff-profiles/pamela-agnew',\n",
       "  'telephone': '703-648-7435'},\n",
       " {'name': 'Sean Kamran Ahdi',\n",
       "  'title': 'Research Geophysicist',\n",
       "  'organization_name': None,\n",
       "  'organization_link': None,\n",
       "  'email': 'sahdi@usgs.gov',\n",
       "  'profile': 'https://usgs.gov/staff-profiles/sean-kamran-ahdi',\n",
       "  'telephone': '303-273-8500'},\n",
       " {'name': 'Elizabeth Ahearn',\n",
       "  'title': 'Lead Hydrologist',\n",
       "  'organization_name': 'New England Water Science Center',\n",
       "  'organization_link': 'https://www.usgs.gov/centers/new-england-water',\n",
       "  'email': 'eaahearn@usgs.gov',\n",
       "  'profile': 'https://usgs.gov/staff-profiles/elizabeth-ahearn',\n",
       "  'telephone': '860-291-6745'},\n",
       " {'name': 'Erik Ahl',\n",
       "  'title': 'Cartographer',\n",
       "  'organization_name': 'National Geospatial Technical Operations Center (NGTOC)',\n",
       "  'organization_link': 'https://www.usgs.gov/core-science-systems/ngp/ngtoc',\n",
       "  'email': 'eahl@usgs.gov',\n",
       "  'profile': 'https://usgs.gov/staff-profiles/erik-ahl',\n",
       "  'telephone': '303-202-4733'},\n",
       " {'name': 'Christina Ahlstrom, Ph.D.',\n",
       "  'title': 'Geneticist',\n",
       "  'organization_name': 'Alaska Science Center',\n",
       "  'organization_link': 'https://www.usgs.gov/centers/asc',\n",
       "  'email': 'cahlstrom@usgs.gov',\n",
       "  'profile': 'https://usgs.gov/staff-profiles/christina-ahlstrom',\n",
       "  'telephone': '907-786-7114'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(profile_records))\n",
    "display(profile_records[10:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ended up running into a number of issues due to the vagaries of trying to do anything over HTTP. The full set of processing would not necessarily complete every time I've tried to run it through, usually due to an HTTP connection pool error of some kind. This is not necessarily the result of some kind of throttling resulting from me sending multiple simultaneous requests but could be introduced by some other bottleneck. I'm hoping that the use of the message queue will help in this as I think we can run things in a guaranteed result mode of some kind, possibly with a \"dead letter queue\" to handle complete failures. For running here, I cache records into pickle files and then start over with those contents if something blows up.\n",
    "\n",
    "The scrape_profile function will return a number of different errors if it encounters problems in processing. For my purposes here, I dump these to a list of errors. In a message queuing architecture, we may want to drop these onto another queue for some other handling. Right now, I'm just ignoring these until I determine what do to about whatever errors were encountered that make the records unusable for our current purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"profile_data.p\"):\n",
    "    profile_data = pickle.load(open(\"profile_data.p\", \"rb\"))\n",
    "else:\n",
    "    profile_data = list()\n",
    "\n",
    "if os.path.exists(\"profile_errors.p\"):\n",
    "    profile_errors = pickle.load(open(\"profile_errors.p\", \"rb\"))\n",
    "else:\n",
    "    profile_errors = list()\n",
    "\n",
    "def profile_accumulator(inventory_record):\n",
    "    scraped_profile = usgs_web.scrape_profile(\n",
    "        inventory_record[\"profile\"],\n",
    "        inventory_content=inventory_record\n",
    "    )\n",
    "    if \"error\" in scraped_profile:\n",
    "        profile_errors.append(inventory_record)\n",
    "    elif \"entity\" in scraped_profile:\n",
    "        profile_data.append(scraped_profile)\n",
    "    else:\n",
    "        profile_errors.append(scraped_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6247/6247 [13:20<00:00,  7.80it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    Parallel(n_jobs=40, prefer=\"threads\")(\n",
    "        delayed(profile_accumulator)\n",
    "        (\n",
    "            i\n",
    "        ) for i in tqdm.tqdm([i for i in profile_records if i[\"profile\"] not in [p[\"entity\"][\"reference\"] for p in profile_data]])\n",
    "    )\n",
    "except Exception as e:\n",
    "    pickle.dump(profile_data, open(\"profile_data.p\", \"wb\"))\n",
    "    pickle.dump(profile_errors, open(\"profile_errors.p\", \"wb\"))\n",
    "    pickle.dump(profile_misses, open(\"profile_misses.p\", \"wb\"))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(profile_data, open(\"profile_data.p\", \"wb\"))\n",
    "pickle.dump(profile_errors, open(\"profile_errors.p\", \"wb\"))\n",
    "pickle.dump(profile_misses, open(\"profile_misses.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting data from profile scraping is now essentially suitable for creation of our final entities and statements/claims. The claims are unique to this particular dataset with other claims coming in from other sources, and so those can be placed directly into a claims index. If this is the first time an entity record has turned up in our system, then this is the best possible record we have for that record. However, if a record already exists for this entity, determined by identifiers, then we will want to evaluate this new information and decide what to do with it. In general, we are treating Profile Page information as pretty much best representation when there is a conflict in other sources. However, with the lack of basic quality control/validation in the Profiles application, we still end up with messiness like extra spaces in names that can't all be reasonably corrected for.\n",
    "\n",
    "One of the main things we will introduce to the entity records through other processes will be additional identifiers matched to the individual. I've been updating the identifiers object in the documents to incorporate these new identifiers, and I've also experimented with going back through related claims on entities as subjects and updated those subject_identifiers to include the full slate of discovered identifiers. This makes queries simpler but is not absolutely necessary as we can start from identifier or come back to identifier in the entity objects and get to all claims based on all available identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "6245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': {'reference': 'https://usgs.gov/staff-profiles/garrett-akie',\n",
       "   'entity_created': '2020-11-20T16:11:49.173829',\n",
       "   'entity_source': 'USGS Profile Page',\n",
       "   'instance_of': 'Person',\n",
       "   'name': 'Garrett Akie',\n",
       "   'url': ['https://usgs.gov/staff-profiles/garrett-akie'],\n",
       "   'identifiers': {'email': 'gakie@usgs.gov', 'orcid': '0000-0002-6356-7106'}},\n",
       "  'claims': [{'claim_created': '2020-11-20T16:11:49.173843',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/garrett-akie',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.173844',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'gakie@usgs.gov',\n",
       "     'orcid': '0000-0002-6356-7106'},\n",
       "    'subject_label': 'Garrett Akie',\n",
       "    'property_label': 'organization affiliation',\n",
       "    'object_instance_of': 'Organization',\n",
       "    'object_label': 'Colorado Water Science Center',\n",
       "    'object_identifiers': {'url': 'https://www.usgs.gov/centers/co-water'}},\n",
       "   {'claim_created': '2020-11-20T16:11:49.173843',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/garrett-akie',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.173844',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'gakie@usgs.gov',\n",
       "     'orcid': '0000-0002-6356-7106'},\n",
       "    'subject_label': 'Garrett Akie',\n",
       "    'property_label': 'job title',\n",
       "    'object_instance_of': 'FieldOfWork',\n",
       "    'object_label': 'Hydrologist'}]},\n",
       " {'entity': {'reference': 'https://usgs.gov/staff-profiles/justin-abel',\n",
       "   'entity_created': '2020-11-20T16:11:49.353630',\n",
       "   'entity_source': 'USGS Profile Page',\n",
       "   'instance_of': 'Person',\n",
       "   'name': 'Justin Abel',\n",
       "   'url': ['https://usgs.gov/staff-profiles/justin-abel'],\n",
       "   'identifiers': {'email': 'jabel@usgs.gov'}},\n",
       "  'claims': [{'claim_created': '2020-11-20T16:11:49.353643',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/justin-abel',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.353645',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'jabel@usgs.gov'},\n",
       "    'subject_label': 'Justin Abel',\n",
       "    'property_label': 'organization affiliation',\n",
       "    'object_instance_of': 'Organization',\n",
       "    'object_label': 'Kansas Water Science Center',\n",
       "    'object_identifiers': {'url': 'https://www.usgs.gov/centers/kswsc'}},\n",
       "   {'claim_created': '2020-11-20T16:11:49.353643',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/justin-abel',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.353645',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'jabel@usgs.gov'},\n",
       "    'subject_label': 'Justin Abel',\n",
       "    'property_label': 'job title',\n",
       "    'object_instance_of': 'FieldOfWork',\n",
       "    'object_label': 'Hydrologic Technician'}]},\n",
       " {'entity': {'reference': 'https://usgs.gov/staff-profiles/richard-g-rich-akins',\n",
       "   'entity_created': '2020-11-20T16:11:49.439821',\n",
       "   'entity_source': 'USGS Profile Page',\n",
       "   'instance_of': 'Person',\n",
       "   'name': 'Richard G (Rich) Akins',\n",
       "   'url': ['https://usgs.gov/staff-profiles/richard-g-rich-akins'],\n",
       "   'identifiers': {'email': 'rakins@usgs.gov'}},\n",
       "  'claims': [{'claim_created': '2020-11-20T16:11:49.439834',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/richard-g-rich-akins',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.439836',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'rakins@usgs.gov'},\n",
       "    'subject_label': 'Richard G (Rich) Akins',\n",
       "    'property_label': 'organization affiliation',\n",
       "    'object_instance_of': 'Organization',\n",
       "    'object_label': 'Central Midwest Water Science Center',\n",
       "    'object_identifiers': {'url': 'https://www.usgs.gov/centers/cm-water'}},\n",
       "   {'claim_created': '2020-11-20T16:11:49.439834',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/richard-g-rich-akins',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.439836',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'rakins@usgs.gov'},\n",
       "    'subject_label': 'Richard G (Rich) Akins',\n",
       "    'property_label': 'job title',\n",
       "    'object_instance_of': 'FieldOfWork',\n",
       "    'object_label': 'Hydrologic Technician'}]},\n",
       " {'entity': {'reference': 'https://usgs.gov/staff-profiles/linda-adams',\n",
       "   'entity_created': '2020-11-20T16:11:49.696497',\n",
       "   'entity_source': 'USGS Profile Page',\n",
       "   'instance_of': 'Person',\n",
       "   'name': 'Linda Adams',\n",
       "   'url': ['https://usgs.gov/staff-profiles/linda-adams'],\n",
       "   'identifiers': {'email': 'ltadams@usgs.gov'}},\n",
       "  'claims': [{'claim_created': '2020-11-20T16:11:49.696512',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/linda-adams',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.696513',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'ltadams@usgs.gov'},\n",
       "    'subject_label': 'Linda Adams',\n",
       "    'property_label': 'organization affiliation',\n",
       "    'object_instance_of': 'Organization',\n",
       "    'object_label': 'Leetown Science Center',\n",
       "    'object_identifiers': {'url': 'https://www.usgs.gov/centers/lsc'}},\n",
       "   {'claim_created': '2020-11-20T16:11:49.696512',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/linda-adams',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.696513',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'ltadams@usgs.gov'},\n",
       "    'subject_label': 'Linda Adams',\n",
       "    'property_label': 'job title',\n",
       "    'object_instance_of': 'FieldOfWork',\n",
       "    'object_label': 'Secretary'}]},\n",
       " {'entity': {'reference': 'https://usgs.gov/staff-profiles/amarys-acosta',\n",
       "   'entity_created': '2020-11-20T16:11:49.919994',\n",
       "   'entity_source': 'USGS Profile Page',\n",
       "   'instance_of': 'Person',\n",
       "   'name': 'Amarys Acosta',\n",
       "   'url': ['https://usgs.gov/staff-profiles/amarys-acosta'],\n",
       "   'identifiers': {'email': 'aacost@usgs.gov'}},\n",
       "  'claims': [{'claim_created': '2020-11-20T16:11:49.920007',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/amarys-acosta',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.920008',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'aacost@usgs.gov'},\n",
       "    'subject_label': 'Amarys Acosta',\n",
       "    'property_label': 'expertise',\n",
       "    'object_instance_of': 'UnlinkedTerm',\n",
       "    'object_qualifier': 'subject personal assertion',\n",
       "    'object_label': 'LDM'},\n",
       "   {'claim_created': '2020-11-20T16:11:49.920007',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/amarys-acosta',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.920008',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'aacost@usgs.gov'},\n",
       "    'subject_label': 'Amarys Acosta',\n",
       "    'property_label': 'expertise',\n",
       "    'object_instance_of': 'UnlinkedTerm',\n",
       "    'object_qualifier': 'subject personal assertion',\n",
       "    'object_label': 'DECODES'},\n",
       "   {'claim_created': '2020-11-20T16:11:49.920007',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/amarys-acosta',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.920008',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'aacost@usgs.gov'},\n",
       "    'subject_label': 'Amarys Acosta',\n",
       "    'property_label': 'expertise',\n",
       "    'object_instance_of': 'UnlinkedTerm',\n",
       "    'object_qualifier': 'subject personal assertion',\n",
       "    'object_label': 'Aquarius'},\n",
       "   {'claim_created': '2020-11-20T16:11:49.920007',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/amarys-acosta',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.920008',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'aacost@usgs.gov'},\n",
       "    'subject_label': 'Amarys Acosta',\n",
       "    'property_label': 'expertise',\n",
       "    'object_instance_of': 'UnlinkedTerm',\n",
       "    'object_qualifier': 'subject personal assertion',\n",
       "    'object_label': 'Surface Water'},\n",
       "   {'claim_created': '2020-11-20T16:11:49.920007',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/amarys-acosta',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.920008',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'aacost@usgs.gov'},\n",
       "    'subject_label': 'Amarys Acosta',\n",
       "    'property_label': 'expertise',\n",
       "    'object_instance_of': 'UnlinkedTerm',\n",
       "    'object_qualifier': 'subject personal assertion',\n",
       "    'object_label': 'Hydroacoustics'},\n",
       "   {'claim_created': '2020-11-20T16:11:49.920007',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/amarys-acosta',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.920008',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'aacost@usgs.gov'},\n",
       "    'subject_label': 'Amarys Acosta',\n",
       "    'property_label': 'expertise',\n",
       "    'object_instance_of': 'UnlinkedTerm',\n",
       "    'object_qualifier': 'subject personal assertion',\n",
       "    'object_label': 'Index Velocity'},\n",
       "   {'claim_created': '2020-11-20T16:11:49.920007',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/amarys-acosta',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.920008',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'aacost@usgs.gov'},\n",
       "    'subject_label': 'Amarys Acosta',\n",
       "    'property_label': 'organization affiliation',\n",
       "    'object_instance_of': 'Organization',\n",
       "    'object_label': 'Oregon Water Science Center',\n",
       "    'object_identifiers': {'url': 'https://www.usgs.gov/centers/or-water'}},\n",
       "   {'claim_created': '2020-11-20T16:11:49.920007',\n",
       "    'claim_source': 'USGS Profile Page',\n",
       "    'reference': 'https://usgs.gov/staff-profiles/amarys-acosta',\n",
       "    'date_qualifier': '2020-11-20T16:11:49.920008',\n",
       "    'subject_instance_of': 'Person',\n",
       "    'subject_identifiers': {'email': 'aacost@usgs.gov'},\n",
       "    'subject_label': 'Amarys Acosta',\n",
       "    'property_label': 'job title',\n",
       "    'object_instance_of': 'FieldOfWork',\n",
       "    'object_label': 'Hydrologic Technician'}]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'profile': 'https://usgs.gov/staff-profiles/travis-owen-culp',\n",
       "  'date_cached': '2020-11-20T16:14:45.008482',\n",
       "  'display_name': 'Travis Owen Culp, MA Leadership',\n",
       "  'profile_image_url': 'https://prd-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/styles/content_grid/public/thumbnails/image/20191209_071005%20%281%29.jpg',\n",
       "  'organization_name': 'Dakota Water Science Center',\n",
       "  'organization_link': 'https://www.usgs.gov/centers/dakota-water',\n",
       "  'email': None,\n",
       "  'orcid': None,\n",
       "  'body_content_links': [],\n",
       "  'expertise': [],\n",
       "  'scraped_body_html': None},\n",
       " {'profile': 'https://usgs.gov/staff-profiles/gregory-shellenbarger',\n",
       "  'date_cached': '2020-11-20T16:25:12.061455',\n",
       "  'display_name': 'Gregory Shellenbarger',\n",
       "  'profile_image_url': None,\n",
       "  'organization_name': None,\n",
       "  'organization_link': None,\n",
       "  'email': None,\n",
       "  'orcid': None,\n",
       "  'body_content_links': [],\n",
       "  'expertise': ['hydrology',\n",
       "   'estuarine ecosystems',\n",
       "   'wetland ecosystems',\n",
       "   'sediment transport',\n",
       "   'health and disease',\n",
       "   'surface water quality'],\n",
       "  'scraped_body_html': None}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(profile_errors))\n",
    "print(len(profile_data))\n",
    "display(profile_data[:5])\n",
    "display(profile_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we scrape up our full set of records, further problems crop up with uniqueness of what should be reasonably persistent unique identifiers (email and orcid). These seem to be due to problems in the information content itself that needs cleanup. If we have all of our data together, we can check for that and make a choice about duplicates here. When we move to a message queue architecture, we will need to introduce this in the logic that decides where and how to route records. Since we will need someone to go in and actually correct something in the underlying records to uniquely identify them to the individual, we should probably put them into their own index so we can expose the records through an application somewhere. To this end, it might be best to put everything that comes out of the scraping process into a queue and then write a lambda handler that checks against the live index of unique entities and sends anything in question to another index.\n",
    "\n",
    "The only problem there is that we could end up with a first-in-wins situation where the first-in record might be innaccurate. In the case shown below with the current dataset, we have an incorrect ORCID entered for one of these two individuals creating the collision. If we go look at the ORCID for 0000-0001-7520-6669, we can see that it applies to Margaret Lamont and not Raymond Carthy, but we have no way of knowing that until we run that check.\n",
    "\n",
    "Another strategy we might employ is to essentially queue up a given tranche of new data coming into our system, check for duplicates like we have here, and then route dups off to their own space and more accurate records into our master set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_orcids = [orcid for orcid, count in Counter([i[\"entity\"][\"identifiers\"][\"orcid\"] for i in profile_data if \"orcid\" in i[\"entity\"][\"identifiers\"]]).items() if count > 1]\n",
    "duplicate_emails = [email for email, count in Counter([i[\"entity\"][\"identifiers\"][\"email\"] for i in profile_data if \"email\" in i[\"entity\"][\"identifiers\"]]).items() if count > 1]\n",
    "\n",
    "unique_profiles = list()\n",
    "held_profiles = list()\n",
    "for item in profile_data:\n",
    "    if \"email\" in item[\"entity\"][\"identifiers\"] and item[\"entity\"][\"identifiers\"][\"email\"] in duplicate_emails:\n",
    "        held_profiles.append(item)\n",
    "    elif \"orcid\" in item[\"entity\"][\"identifiers\"] and item[\"entity\"][\"identifiers\"][\"orcid\"] in duplicate_orcids:\n",
    "        held_profiles.append(item)\n",
    "    else:\n",
    "        unique_profiles.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(unique_profiles, open(\"unique_profiles.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'reference': 'https://usgs.gov/staff-profiles/raymond-carthy',\n",
       "  'entity_created': '2020-11-20T16:13:06.867421',\n",
       "  'entity_source': 'USGS Profile Page',\n",
       "  'instance_of': 'Person',\n",
       "  'name': 'Raymond Carthy, Ph.D.',\n",
       "  'url': ['https://usgs.gov/staff-profiles/raymond-carthy',\n",
       "   'http://link.springer.com/article/10.1007/s12237-013-9741-x'],\n",
       "  'identifiers': {'email': 'rayc@usgs.gov', 'orcid': '0000-0001-7520-6669'}},\n",
       " {'reference': 'https://usgs.gov/staff-profiles/margaret-lamont',\n",
       "  'entity_created': '2020-11-20T16:18:10.802131',\n",
       "  'entity_source': 'USGS Profile Page',\n",
       "  'instance_of': 'Person',\n",
       "  'name': 'Margaret Lamont, Ph.D.',\n",
       "  'url': ['https://usgs.gov/staff-profiles/margaret-lamont'],\n",
       "  'identifiers': {'email': 'mlamont@usgs.gov',\n",
       "   'orcid': '0000-0001-7520-6669'}}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i[\"entity\"] for i in held_profiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
